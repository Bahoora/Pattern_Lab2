{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m_y1iEwjFB0",
        "outputId": "16c2e927-b453-4cd5-bcb2-f1f3bd98435e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/surajghuwalewala/ham1000-segmentation-and-classification?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.59G/2.59G [00:24<00:00, 113MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/surajghuwalewala/ham1000-segmentation-and-classification/versions/2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"surajghuwalewala/ham1000-segmentation-and-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "mOcVCUXXlU3h"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/surajghuwalewala/ham1000-segmentation-and-classification/versions/2\"\n",
        "\n",
        "# List files in the dataset directory\n",
        "print(\"Dataset contents:\", os.listdir(dataset_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK5jilkjmmNJ",
        "outputId": "aa7be4b9-d6d0-40c3-f5dd-adbb39819924"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset contents: ['images', 'GroundTruth.csv', 'masks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset path (after downloading)\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/surajghuwalewala/ham1000-segmentation-and-classification/versions/2\"\n",
        "\n",
        "# Load the metadata file (assumed to be in CSV format)\n",
        "metadata_path = os.path.join(dataset_path, \"GroundTruth.csv\")  # Adjust filename if needed\n",
        "df = pd.read_csv(metadata_path)\n",
        "\n",
        "# Ensure you have the required columns\n",
        "print(df.head())\n",
        "\n",
        "# Convert one-hot encoding to single class column\n",
        "df['dx'] = df[['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']].idxmax(axis=1)\n",
        "\n",
        "# Now apply label mapping\n",
        "label_mapping = {\"MEL\": 0, \"NV\": 1, \"BCC\": 2, \"AKIEC\": 3, \"BKL\": 4, \"DF\": 5, \"VASC\": 6}\n",
        "df['dx'] = df['dx'].map(label_mapping)\n",
        "df = df.dropna(subset=['dx'])  # Remove rows with missing labels\n",
        "\n",
        "# Define the image directory\n",
        "image_dir = os.path.join(dataset_path, \"images\")  # Adjust if needed\n",
        "\n",
        "# Splitting Data Stratified\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['dx'], random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['dx'], random_state=42)\n",
        "\n",
        "# Function to move images to appropriate folders\n",
        "def move_images(df, source_dir, dest_dir):\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "    for img_name in df['image']:\n",
        "        src = os.path.join(source_dir, img_name + \".jpg\")  # Adjust extension if needed\n",
        "        dest = os.path.join(dest_dir, img_name + \".jpg\")\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy(src, dest)\n",
        "\n",
        "# Create directories\n",
        "move_images(train_df, image_dir, \"dataset/train\")\n",
        "move_images(val_df, image_dir, \"dataset/val\")\n",
        "move_images(test_df, image_dir, \"dataset/test\")\n",
        "\n",
        "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAiG8deZlcAE",
        "outputId": "bfcc338d-a0fe-4bb1-eb56-9ae09f2e38b3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          image  MEL   NV  BCC  AKIEC  BKL   DF  VASC\n",
            "0  ISIC_0024306  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
            "1  ISIC_0024307  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
            "2  ISIC_0024308  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
            "3  ISIC_0024309  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
            "4  ISIC_0024310  1.0  0.0  0.0    0.0  0.0  0.0   0.0\n",
            "Train size: 7210, Val size: 802, Test size: 2003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate images between splits to identify leakage\n",
        "def check_duplicates(df1, df2, key_column):\n",
        "    common = set(df1[key_column]).intersection(set(df2[key_column]))\n",
        "    return len(common), common\n",
        "\n",
        "key_column = \"image\"  #ID\n",
        "dup_train_test, dup_train_test_samples = check_duplicates(train_df, test_df, key_column)\n",
        "dup_train_val, dup_train_val_samples = check_duplicates(train_df, val_df, key_column)\n",
        "dup_val_test, dup_val_test_samples = check_duplicates(val_df, test_df, key_column)\n",
        "\n",
        "print(f\"Duplicates between Train & Test: {dup_train_test}\")\n",
        "print(f\"Duplicates between Train & Validation: {dup_train_val}\")\n",
        "print(f\"Duplicates between Validation & Test: {dup_val_test}\")\n",
        "\n",
        "# If duplicates exist, remove them from the test set\n",
        "if dup_train_test > 0:\n",
        "    test_df = test_df[~test_df[key_column].isin(dup_train_test_samples)]\n",
        "if dup_train_val > 0:\n",
        "    val_df = val_df[~val_df[key_column].isin(dup_train_val_samples)]\n",
        "if dup_val_test > 0:\n",
        "    test_df = test_df[~test_df[key_column].isin(dup_val_test_samples)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFFkL17ZvnmI",
        "outputId": "105469e0-aafb-4803-f308-496067fa85ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicates between Train & Test: 0\n",
            "Duplicates between Train & Validation: 0\n",
            "Duplicates between Validation & Test: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for patient-level leakage\n",
        "if 'patient_id' in df.columns:\n",
        "    train_patients = set(train_df['patient_id'])\n",
        "    test_patients = set(test_df['patient_id'])\n",
        "    val_patients = set(val_df['patient_id'])\n",
        "\n",
        "    overlap_train_test = train_patients.intersection(test_patients)\n",
        "    overlap_train_val = train_patients.intersection(val_patients)\n",
        "    overlap_val_test = val_patients.intersection(test_patients)\n",
        "\n",
        "    print(f\"Patient overlap between Train & Test: {len(overlap_train_test)}\")\n",
        "    print(f\"Patient overlap between Train & Validation: {len(overlap_train_val)}\")\n",
        "    print(f\"Patient overlap between Validation & Test: {len(overlap_val_test)}\")\n",
        "\n",
        "    # Remove patients from test/val that are also in train\n",
        "    test_df = test_df[~test_df['patient_id'].isin(overlap_train_test)]\n",
        "    val_df = val_df[~val_df['patient_id'].isin(overlap_train_val)]\n"
      ],
      "metadata": {
        "id": "SZNy5HzLvvQ6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "\n",
        "#SOMETHING HAPPENS HERE THAT CAUSES ALL IMAGES TO BE CLASSIFIED AS CORRUPT, STILL NEED TO FIX IT\n",
        "# # Function to check if an image is corrupt\n",
        "# # def is_corrupt(image_path):\n",
        "# #     try:\n",
        "# #         with Image.open(image_path) as img:\n",
        "# #             img.verify()  # Check if image is valid\n",
        "# #         return False  # Image is valid\n",
        "# #     except Exception:\n",
        "# #         return True  # Image is corrupt\n",
        "# def is_corrupt(image_path):\n",
        "#     try:\n",
        "#         with Image.open(image_path) as img:\n",
        "#             img.load()  # Load image instead of just verify()\n",
        "#         return False\n",
        "#     except Exception as e:\n",
        "#         print(f\"Corrupt image detected: {image_path} - {e}\")\n",
        "#         return True\n",
        "\n",
        "\n",
        "# # Remove corrupt images from dataset\n",
        "# def remove_corrupt_images(df, image_dir, image_column):\n",
        "#     valid_images = []\n",
        "#     for img_name in df[image_column]:\n",
        "#         img_path = os.path.join(image_dir, img_name)\n",
        "#         if os.path.exists(img_path) and not is_corrupt(img_path):\n",
        "#             valid_images.append(img_name)\n",
        "\n",
        "#     return df[df[image_column].isin(valid_images)]\n",
        "\n",
        "# train_df = remove_corrupt_images(train_df, image_dir, key_column)\n",
        "# val_df = remove_corrupt_images(val_df, image_dir, key_column)\n",
        "# test_df = remove_corrupt_images(test_df, image_dir, key_column)\n"
      ],
      "metadata": {
        "id": "PdkSZ7EovybG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Image augmentation functions\n",
        "def random_rotation(image):\n",
        "    return image.rotate(random.uniform(-30, 30))\n",
        "\n",
        "def random_flip(image):\n",
        "    if random.random() > 0.5:\n",
        "        return image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    return image\n",
        "\n",
        "def random_crop(image, output_size=(224, 224)):\n",
        "    width, height = image.size\n",
        "    left = random.randint(0, width // 4)\n",
        "    top = random.randint(0, height // 4)\n",
        "    right = width - random.randint(0, width // 4)\n",
        "    bottom = height - random.randint(0, height // 4)\n",
        "    return image.crop((left, top, right, bottom)).resize(output_size)\n",
        "\n",
        "# Apply augmentations and save\n",
        "# def preprocess_and_save(df, source_dir, dest_dir, image_column):\n",
        "#     os.makedirs(dest_dir, exist_ok=True)\n",
        "#     saved_count = 0\n",
        "#     failed_count = 0\n",
        "#     for img_name in df[image_column]:\n",
        "#         img_name = str(img_name) + \".jpg\"\n",
        "#         src = os.path.join(source_dir, img_name)\n",
        "#         dest = os.path.join(dest_dir, img_name)\n",
        "\n",
        "#         if os.path.exists(src):\n",
        "#           try:\n",
        "#             with Image.open(src) as img:\n",
        "\n",
        "#                 img = img.convert(\"RGB\")  # Ensure image is in correct format\n",
        "#                 img = random_rotation(img)\n",
        "#                 img = random_flip(img)\n",
        "#                 img = random_crop(img, output_size=(224, 224))  # Resize to 224x224\n",
        "#                 img.save(dest)\n",
        "#                 saved_count += 1\n",
        "#           except Exception as e:\n",
        "#                 print(f\"❌ Error processing {src}: {e}\")\n",
        "#                 failed_count += 1\n",
        "#         else:\n",
        "#             failed_count += 1\n",
        "#     print(f\"✅ Successfully saved: {saved_count}, ❌ Failed: {failed_count}\")\n",
        "# def preprocess_and_save(df, source_dir, dest_dir, image_column, label_column):\n",
        "#     os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "#     saved_count, failed_count = 0, 0\n",
        "#     for _, row in df.iterrows():\n",
        "#         img_name = str(row[image_column]) + \".jpg\"  # Ensure extension\n",
        "#         label = str(row[label_column])  # Get class label\n",
        "#         class_dir = os.path.join(dest_dir, label)  # Class subfolder\n",
        "\n",
        "#         os.makedirs(class_dir, exist_ok=True)  # Create class folder if not exists\n",
        "\n",
        "#         src = os.path.join(source_dir, img_name)\n",
        "#         dest = os.path.join(class_dir, img_name)\n",
        "\n",
        "#         if os.path.exists(src):\n",
        "#             try:\n",
        "#                 with Image.open(src) as img:\n",
        "#                     img = img.convert(\"RGB\")\n",
        "#                     img = random_rotation(img)\n",
        "#                     img = random_flip(img)\n",
        "#                     img = random_crop(img, output_size=(224, 224))\n",
        "#                     img.save(dest)\n",
        "#                     saved_count += 1\n",
        "\n",
        "#                     # Print every 100 images for debugging\n",
        "#                     if saved_count % 100 == 0:\n",
        "#                         print(f\"✅ Processed {saved_count}/{len(df)} images\")\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"❌ Error processing {src}: {e}\")\n",
        "#                 failed_count += 1\n",
        "#         else:\n",
        "#             failed_count += 1\n",
        "\n",
        "#     print(f\"✅ Successfully saved: {saved_count}, ❌ Failed: {failed_count}\")\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def preprocess_and_save(df, source_dir, dest_dir, image_column):\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "    missing_count = 0\n",
        "    processed_count = 0\n",
        "\n",
        "    for img_name in df[image_column]:\n",
        "        src = os.path.join(source_dir, img_name)  # No extension added yet\n",
        "        dest = os.path.join(dest_dir, img_name)\n",
        "\n",
        "        # Check if the file exists with different extensions\n",
        "        if not os.path.exists(src):\n",
        "            if os.path.exists(src + \".jpg\"):\n",
        "                src += \".jpg\"\n",
        "                dest += \".jpg\"\n",
        "            elif os.path.exists(src + \".png\"):\n",
        "                src += \".png\"\n",
        "                dest += \".png\"\n",
        "            else:\n",
        "                print(f\"❌ Missing during preprocessing: {src}\")\n",
        "                missing_count += 1\n",
        "                continue  # Skip to the next image\n",
        "\n",
        "        # Process and save image\n",
        "        try:\n",
        "            with Image.open(src) as img:\n",
        "                img = img.convert(\"RGB\")  # Ensure correct format\n",
        "                img.save(dest)\n",
        "                processed_count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {src}: {e}\")\n",
        "\n",
        "    print(f\"✅ Processed images: {processed_count}\")\n",
        "    print(f\"❌ Total missing images during preprocessing: {missing_count}\")\n",
        "\n",
        "# Re-run for the test set\n",
        "preprocess_and_save(test_df, \"dataset/test\", \"dataset/preprocessed_test\", \"image\")\n",
        "\n",
        "\n",
        "# Apply preprocessing\n",
        "preprocess_and_save(train_df, \"dataset/train\", \"dataset/preprocessed_train\", \"image\", )\n",
        "preprocess_and_save(test_df, \"dataset/test\", \"dataset/preprocessed_test\", \"image\",)\n",
        "preprocess_and_save(val_df, \"dataset/val\", \"dataset/preprocessed_val\", \"image\",)\n",
        "\n",
        "print(\"Preprocessing complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvIUG6Lxv2Nk",
        "outputId": "e6e3e9e2-a2bf-450f-947f-d830ecd0418b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Processed images: 2003\n",
            "❌ Total missing images during preprocessing: 0\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031146\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030109\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028940\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025857\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034201\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025726\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027549\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025852\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025402\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028841\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024786\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028928\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030212\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025941\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031169\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025888\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024453\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027646\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031651\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024519\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029966\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027287\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027356\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028869\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029958\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024459\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030093\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027424\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028791\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027431\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028058\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031565\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034091\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025062\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026838\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027116\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028713\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026692\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024607\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029410\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030954\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033926\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026967\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027403\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033260\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031636\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028680\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025293\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026468\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028303\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030725\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030665\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027535\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026100\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031646\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033326\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029792\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027884\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031454\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029884\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031273\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024981\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025947\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031527\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031027\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031774\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028482\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031513\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028369\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031401\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033082\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032591\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025490\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033350\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033446\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026876\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028018\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027673\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025840\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029553\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031475\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030614\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024980\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027118\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027895\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030201\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033770\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026094\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030193\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026842\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033369\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033230\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033993\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032596\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026936\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024989\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030581\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024882\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026845\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028092\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032522\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029938\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033240\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026631\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024481\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032168\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030421\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027010\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027798\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029946\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033611\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030155\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033875\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025424\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031064\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034173\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028161\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032422\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030881\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030643\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026047\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033484\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034066\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034019\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028295\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024742\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032147\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025654\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025867\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034096\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033111\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030158\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025560\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028630\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030059\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030596\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029690\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028079\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028951\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030327\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025776\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024500\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027666\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029842\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028240\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024460\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024758\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027140\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028398\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028140\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033216\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033034\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032179\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027545\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031804\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029336\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027487\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030346\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025567\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034315\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032115\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028456\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025926\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033163\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025739\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029927\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026075\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027288\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026776\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032603\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025710\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030486\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027257\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026235\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032393\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026256\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031035\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028074\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033328\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026371\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027083\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030149\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025805\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032782\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034058\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031566\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032578\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029311\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033220\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024564\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027490\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030013\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033555\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025733\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024479\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033102\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029753\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029388\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033465\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029319\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030394\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032772\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029406\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025204\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030985\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026872\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031659\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025813\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028949\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030956\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031625\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027944\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024949\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033286\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033495\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025103\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030843\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025280\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032339\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029784\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030652\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031223\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024545\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030849\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032939\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032885\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032497\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026465\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030001\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029582\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027241\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033928\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033202\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027252\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027444\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030648\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025799\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027100\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027699\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026493\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033779\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026818\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031542\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027651\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026679\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029180\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030903\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027011\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032818\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025634\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030963\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028477\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029648\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030965\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024765\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026650\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032905\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033718\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028542\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027911\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032202\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026062\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024667\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031023\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026095\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029604\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033571\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025051\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026958\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028902\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027509\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033487\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025310\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025373\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028429\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027698\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024960\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024930\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027526\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027790\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032951\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029462\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033810\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030912\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026136\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025037\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030829\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028535\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024576\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033609\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027038\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034149\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029511\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032812\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024713\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031734\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030718\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033759\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033823\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025008\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033382\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030831\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028738\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027312\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031017\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029123\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034281\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027638\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026659\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026596\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033322\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025577\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033682\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032637\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024885\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033671\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028396\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026856\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034101\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028209\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025707\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030589\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028141\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031248\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033475\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034231\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028663\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029892\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033138\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026364\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028524\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028397\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030042\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032623\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031569\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032073\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033204\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033766\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027998\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025195\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029432\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034212\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025069\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026275\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025059\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031081\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028473\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026029\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028172\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030943\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030200\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030320\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033596\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024311\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030106\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034282\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027900\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028412\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032541\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027556\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029738\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026240\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029153\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032490\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026394\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027194\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031085\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029363\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031508\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033635\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030871\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029993\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034147\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033815\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027388\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028682\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027617\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034002\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025084\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028246\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026767\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024936\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032404\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030961\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032619\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024656\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028880\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024840\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033095\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028522\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030119\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033720\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031413\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026429\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026795\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032679\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027244\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033359\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030964\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027306\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032239\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030113\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030594\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033544\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026956\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028162\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026452\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032506\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033514\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027815\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027875\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032427\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028070\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034199\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026937\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034218\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033093\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027409\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033901\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025191\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032170\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033986\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031858\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024622\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028787\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025043\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026210\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025574\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033176\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024357\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031946\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032190\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032096\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027711\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029029\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026489\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027804\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026864\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030381\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030798\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026229\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031902\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027580\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029346\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033647\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025798\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029975\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028306\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028660\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026527\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029031\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027947\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031922\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026089\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025170\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025329\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026808\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025012\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025762\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032580\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027730\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027829\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026515\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031966\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026853\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025115\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024954\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030680\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024898\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025317\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032050\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029059\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032969\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025446\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031451\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031665\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028506\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032635\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029771\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026689\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026315\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029295\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025647\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029248\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025488\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032661\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028547\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027927\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028390\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025228\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024512\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026026\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028837\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028354\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027423\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025082\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025921\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026645\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030559\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028933\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031465\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034107\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028296\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028075\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032127\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026868\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029273\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033114\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024929\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034242\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028520\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030576\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030853\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031564\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032647\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031225\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032710\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029702\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033384\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028046\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032338\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026068\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033631\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030861\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028923\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028511\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026141\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028427\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024592\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029057\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024558\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034232\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033920\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027395\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028806\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029845\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033352\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030505\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031292\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028444\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033100\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027211\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025111\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027693\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030854\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029876\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032459\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027746\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031144\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033319\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027254\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031284\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029246\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025760\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028801\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025410\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031217\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029869\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029661\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026120\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032224\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025515\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027336\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024994\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033347\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027600\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032850\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026678\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024524\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029696\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028002\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033493\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025836\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026483\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027366\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032711\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027041\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029003\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024629\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033896\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030132\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033997\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025806\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026733\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031226\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028406\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032691\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027234\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026511\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025991\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027435\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031578\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027747\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032150\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026768\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026002\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028410\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026660\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030443\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032090\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030546\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024921\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026383\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030555\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028844\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029347\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029786\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033974\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028235\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027060\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030308\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028845\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031655\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026167\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031892\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026544\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027375\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033683\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028697\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026831\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028644\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028171\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027204\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033403\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028260\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027543\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032430\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031612\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025616\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026191\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033558\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031602\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032067\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024653\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034249\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032930\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030350\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025795\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026674\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033634\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033112\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030230\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031934\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026110\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031061\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026304\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028283\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027370\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028032\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032305\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027406\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025579\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027058\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029955\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031405\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027832\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026064\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030494\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027708\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025535\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033048\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027833\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030257\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033070\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034215\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029023\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030281\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028553\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031758\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027226\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025860\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026675\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025296\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028312\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031388\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031737\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028921\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032481\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027811\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032815\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033565\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033498\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030129\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033156\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031551\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026582\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030564\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034294\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033999\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027392\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029820\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028207\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025878\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031944\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033606\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028180\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034304\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028057\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024604\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028261\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027107\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031742\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026376\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031060\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033272\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025827\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031231\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027950\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027642\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027080\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034001\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032587\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029049\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032908\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026040\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033796\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027948\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028438\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026391\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027061\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025555\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030363\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028862\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032263\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026531\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028401\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030288\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032704\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028540\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025281\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030959\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030372\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024632\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027472\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026593\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029474\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033040\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033648\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030630\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026867\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024809\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032819\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033924\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029644\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025476\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032960\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025466\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033137\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025873\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028675\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032737\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032636\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034240\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031374\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027570\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032970\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032309\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028552\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026820\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030945\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033776\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027894\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033334\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024556\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034045\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024573\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034273\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029959\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027578\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031224\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029671\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033614\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027880\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030348\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028146\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030974\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030812\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026165\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029040\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029385\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0030602\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033562\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032297\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028733\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031485\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0025781\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029741\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026532\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029581\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0027659\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024546\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0024755\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028523\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0031567\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0029643\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0033949\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0032267\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0028364\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026500\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026450\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0034137\n",
            "❌ Missing during preprocessing: dataset/train/ISIC_0026634\n",
            "✅ Processed images: 7210\n",
            "❌ Total missing images during preprocessing: 802\n",
            "✅ Processed images: 2003\n",
            "❌ Total missing images during preprocessing: 0\n",
            "✅ Processed images: 802\n",
            "❌ Total missing images during preprocessing: 0\n",
            "Preprocessing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "test_images = os.listdir(\"dataset/test\")\n",
        "print(f\"Total test images on disk: {len(test_images)}\")\n",
        "print(\"First 5 images:\", test_images[:5])\n",
        "print(test_df['image'].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kYmbB1MoSLW",
        "outputId": "52f0d950-e5b5-48f0-e5e3-2a190e155ad7"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total test images on disk: 2003\n",
            "First 5 images: ['ISIC_0026217.jpg', 'ISIC_0026557.jpg', 'ISIC_0032290.jpg', 'ISIC_0028825.jpg', 'ISIC_0030446.jpg']\n",
            "6357    ISIC_0030663.jpg\n",
            "6963    ISIC_0031269.jpg\n",
            "3579    ISIC_0027885.jpg\n",
            "4042    ISIC_0028348.jpg\n",
            "7871    ISIC_0032177.jpg\n",
            "Name: image, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set_images = set(os.listdir(\"dataset/test\"))  # Images in actual test folder\n",
        "df_images = set(test_df['image'])  # Images listed in CSV\n",
        "\n",
        "missing_images = df_images - test_set_images  # Files in CSV but NOT on disk\n",
        "extra_images = test_set_images - df_images  # Files on disk but NOT in CSV\n",
        "\n",
        "print(f\"❌ Images in CSV but missing on disk: {len(missing_images)}\")\n",
        "print(f\"✅ Extra images found in folder but NOT in CSV: {len(extra_images)}\")\n",
        "\n",
        "# Show some missing files\n",
        "print(\"Some missing files:\", list(missing_images)[:10])\n",
        "print(\"Some extra files:\", list(extra_images)[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh63RsedoeYr",
        "outputId": "9efd95cf-41fa-4dab-9d93-c0d59afe137e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Images in CSV but missing on disk: 0\n",
            "✅ Extra images found in folder but NOT in CSV: 0\n",
            "Some missing files: []\n",
            "Some extra files: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_test_images = os.listdir(\"dataset/preprocessed_test\")\n",
        "print(f\"Total preprocessed test images: {len(preprocessed_test_images)}\")\n",
        "import os\n",
        "\n",
        "test_subfolders = [f.name for f in os.scandir(\"dataset/preprocessed_test\") if f.is_dir()]\n",
        "print(\"Subfolders inside preprocessed_test:\", test_subfolders)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EvEUzvoouYT",
        "outputId": "b33295c4-7c22-46f1-c4ed-f86bbb919c63"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total preprocessed test images: 2010\n",
            "Subfolders inside preprocessed_test: ['5', '4', '3', '0', '6', '1', '2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.dataframe.iloc[idx]['image']\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        label = self.dataframe.iloc[idx]['dx']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Use custom dataset\n",
        "test_dataset = CustomDataset(test_df, \"dataset/preprocessed_test\", transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "URQQbf_Io1VX"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# print(os.listdir(\"dataset/preprocessed_train\"))  # Should list class names\n",
        "# train_dataset = datasets.ImageFolder(root=\"dataset/preprocessed_train\", transform=transform)\n",
        "# print(\"Train samples:\", len(train_dataset))  # Should print correct count\n"
      ],
      "metadata": {
        "id": "d_EIarace-hg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# missing_count = 0\n",
        "# for img_name in train_df['image']:\n",
        "#     img_name = str(img_name) + \".jpg\"  # Ensure extension\n",
        "#     img_path = os.path.join(image_dir, img_name)\n",
        "#     if not os.path.exists(img_path):\n",
        "#         print(f\"❌ Missing: {img_path}\")\n",
        "#         missing_count += 1\n",
        "\n",
        "# print(f\"Total missing images in train_df: {missing_count}\")\n"
      ],
      "metadata": {
        "id": "lldDQgyXcmaG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# source_dir = \"/root/.cache/kagglehub/datasets/surajghuwalewala/ham1000-segmentation-and-classification/versions/2/images\"\n",
        "# print(\"Checking source directory:\", source_dir)\n",
        "# print(\"Total files in source:\", len(os.listdir(source_dir)))\n",
        "\n",
        "# # Check first few file names\n",
        "# print(\"First 5 files:\", os.listdir(source_dir)[:5])\n"
      ],
      "metadata": {
        "id": "UunLsuU2cHyg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Define the CNN model (Modifying from Keras to PyTorch)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 28 * 28, 128),  # Adjust based on input size\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)  # 7 classes in HAM10000\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "# Check model parameters\n",
        "model = CNN(num_classes=7)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params}\")  # Ensure it's < 60M\n",
        "\n",
        "# If needed, move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8v1HYl42KLe",
        "outputId": "cfec9bb0-942d-45dc-c062-71370b23deed"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 6479879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# image_dir = \"/root/.cache/kagglehub/datasets/surajghuwalewala/ham1000-segmentation-and-classification/versions/2/images\"\n",
        "\n",
        "# missing_count = 0\n",
        "# for img_name in df['image']:\n",
        "#     img_path = os.path.join(image_dir, img_name + \".jpg\")  # Adjust extension if needed\n",
        "#     if not os.path.exists(img_path):\n",
        "#         print(f\"❌ Missing: {img_path}\")\n",
        "#         missing_count += 1\n",
        "\n",
        "# print(f\"Total missing images: {missing_count}\")\n"
      ],
      "metadata": {
        "id": "HA5eiqoSPFSj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# image_dir = \"/root/.cache/kagglehub/datasets/surajghuwalewala/ham1000-segmentation-and-classification/versions/2/images\"\n",
        "\n",
        "# if os.path.exists(image_dir):\n",
        "#     print(\"✅ Image directory exists\")\n",
        "#     print(f\"Total files in folder: {len(os.listdir(image_dir))}\")  # Count files\n",
        "# else:\n",
        "#     print(\"❌ Image directory does NOT exist! Check the path.\")\n"
      ],
      "metadata": {
        "id": "6XjDPG8yPy-Q"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Before split - dataset size: {len(df)}\")\n",
        "# train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['dx'], random_state=42)\n",
        "# print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
        "# print(f\"Before removing corrupt images - Train size: {len(train_df)}\")\n"
      ],
      "metadata": {
        "id": "zXlp3LR8WtQE"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corrupt_count = 0\n",
        "# for img_name in train_df['image']:\n",
        "#     img_path = os.path.join(image_dir, img_name)\n",
        "#     if not os.path.exists(img_path) or is_corrupt(img_path):\n",
        "#         corrupt_count += 1\n",
        "\n",
        "# print(f\"Total images in train set: {len(train_df)}\")\n",
        "# print(f\"Images marked as corrupt: {corrupt_count}\")\n"
      ],
      "metadata": {
        "id": "e_MzVGvZXetE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# existing_images = [img for img in train_df['image'] if os.path.exists(os.path.join(image_dir, img))]\n",
        "# print(f\"Existing images in train set: {len(existing_images)}\")\n"
      ],
      "metadata": {
        "id": "5hy6dRC6XjyV"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset loading\n",
        "class SkinLesionDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.dataframe.iloc[idx]['image']\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        label = self.dataframe.iloc[idx]['dx']  # Replace with correct column name\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = SkinLesionDataset(train_df, \"dataset/preprocessed_train\", transform=transform)\n",
        "val_dataset = SkinLesionDataset(val_df, \"dataset/preprocessed_val\", transform=transform)\n",
        "test_dataset = SkinLesionDataset(test_df, \"dataset/preprocessed_test\", transform=transform)\n",
        "\n",
        "\n",
        "# print(df['dx'].unique())  # Check if labels are properly assigned\n",
        "# print(df['dx'].isnull().sum())  # Check if there are missing values\n",
        "\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "cH3j316F2R6_"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# train_path = \"dataset/preprocessed_train\"\n",
        "# print(\"Checking preprocessed train dataset folder...\")\n",
        "# if not os.path.exists(train_path):\n",
        "#     print(\"❌ Folder does NOT exist! Check if preprocessing worked.\")\n",
        "# else:\n",
        "#     print(\"✅ Folder exists. Number of files:\", len(os.listdir(train_path)))\n"
      ],
      "metadata": {
        "id": "yquz_7ycaAln"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess_and_save(train_df, image_dir, \"dataset/preprocessed_train\", \"image\")\n",
        "# preprocess_and_save(val_df, image_dir, \"dataset/preprocessed_val\", \"image\")\n",
        "# preprocess_and_save(test_df, image_dir, \"dataset/preprocessed_test\", \"image\")\n"
      ],
      "metadata": {
        "id": "mkS-qD-VaF6r"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# missing_count = 0\n",
        "# for img_name in train_df['image']:\n",
        "#     img_path = os.path.join(\"dataset/preprocessed_train\", img_name +\".jpg\")  # Make sure there's an extension\n",
        "#     if not os.path.exists(img_path):\n",
        "#         print(f\"❌ Missing: {img_path}\")\n",
        "#         missing_count += 1\n",
        "\n",
        "# print(f\"Total missing images: {missing_count}\")\n"
      ],
      "metadata": {
        "id": "rIblpq4waILK"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "preprocessed_train_dir = \"dataset/preprocessed_train\"  # Adjust path if needed\n",
        "print(\"Checking preprocessed train directory:\", preprocessed_train_dir)\n",
        "print(\"Total processed images:\", len(os.listdir(preprocessed_train_dir)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tUpSo0beYGe",
        "outputId": "a2c90513-eab2-4486-9aa4-36b814de7566"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking preprocessed train directory: dataset/preprocessed_train\n",
            "Total processed images: 7217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = datasets.ImageFolder(root=\"dataset/preprocessed_train\", transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Check number of images loaded\n",
        "print(\"Train samples:\", len(train_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow4vvvnreeMX",
        "outputId": "42dd67ff-1630-4fa4-f60e-ef74bc1f2716"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 7210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_acc = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "print(\"Training Complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB1IaHGN2T5m",
        "outputId": "8cddfe28-7bb9-47bc-f786-562df1ca88e5"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.0267, Train Accuracy: 0.6684\n",
            "Epoch 2/10, Loss: 0.9272, Train Accuracy: 0.6741\n",
            "Epoch 3/10, Loss: 0.8796, Train Accuracy: 0.6814\n",
            "Epoch 4/10, Loss: 0.8575, Train Accuracy: 0.6864\n",
            "Epoch 5/10, Loss: 0.8337, Train Accuracy: 0.6877\n",
            "Epoch 6/10, Loss: 0.8140, Train Accuracy: 0.6992\n",
            "Epoch 7/10, Loss: 0.7950, Train Accuracy: 0.6999\n",
            "Epoch 8/10, Loss: 0.7873, Train Accuracy: 0.7015\n",
            "Epoch 9/10, Loss: 0.7611, Train Accuracy: 0.7119\n",
            "Epoch 10/10, Loss: 0.7385, Train Accuracy: 0.7178\n",
            "Training Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missing_files = [img for img in test_df['image'] if not os.path.exists(os.path.join(test_image_dir, img))]\n",
        "\n",
        "print(f\"Total missing images: {len(missing_files)}\")\n",
        "if missing_files:\n",
        "    print(\"Some missing images:\", missing_files[:10])  # Print the first 10 missing files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju1yY0sej1Sy",
        "outputId": "fef523bb-eef2-4412-a583-2e281b774d12"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total missing images: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total missing images: {sum([not os.path.exists(os.path.join('dataset/test', img)) for img in test_df['image']])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbHtAZZ3nyBf",
        "outputId": "dda36d64-8b6e-4403-f571-6a847991e8d6"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total missing images: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Compute Accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Compute Precision, Recall, F1-score\n",
        "report = classification_report(y_true, y_pred, target_names=[\"MEL\", \"NV\", \"BCC\", \"AKIEC\", \"BKL\", \"DF\", \"VASC\"])\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ITOLW47jt9h",
        "outputId": "5bcda73c-79d4-4a0b-b3f0-cfd23a4dfc64"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6096\n",
            "Confusion Matrix:\n",
            " [[   0  195    0    0    0    0   28]\n",
            " [   0 1215    0    0    1    0  125]\n",
            " [   0  100    0    0    0    0    3]\n",
            " [   0   64    0    0    0    0    1]\n",
            " [   0  215    0    0    0    0    5]\n",
            " [   0   22    0    0    0    0    1]\n",
            " [   0   22    0    0    0    0    6]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         MEL       0.00      0.00      0.00       223\n",
            "          NV       0.66      0.91      0.77      1341\n",
            "         BCC       0.00      0.00      0.00       103\n",
            "       AKIEC       0.00      0.00      0.00        65\n",
            "         BKL       0.00      0.00      0.00       220\n",
            "          DF       0.00      0.00      0.00        23\n",
            "        VASC       0.04      0.21      0.06        28\n",
            "\n",
            "    accuracy                           0.61      2003\n",
            "   macro avg       0.10      0.16      0.12      2003\n",
            "weighted avg       0.44      0.61      0.51      2003\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}