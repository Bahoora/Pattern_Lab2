{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_B2ozjzccU7",
        "outputId": "cef8ba2a-d98a-4d45-c99b-bb629bc31cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/surajghuwalewala/ham1000-segmentation-and-classification/versions/2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"surajghuwalewala/ham1000-segmentation-and-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/surajghuwalewala/ham1000-segmentation-and-classification/versions/2\"\n",
        "\n",
        "# List files in the dataset directory\n",
        "print(\"Dataset contents:\", os.listdir(dataset_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFIDPQmHcm7x",
        "outputId": "4c18dbfb-ce24-468f-be2d-045489659adb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset contents: ['masks', 'GroundTruth.csv', 'images']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset path (after downloading)\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/surajghuwalewala/ham1000-segmentation-and-classification/versions/2\"\n",
        "\n",
        "# Load the metadata file (assumed to be in CSV format)\n",
        "metadata_path = os.path.join(dataset_path, \"GroundTruth.csv\")  # Adjust filename if needed\n",
        "df = pd.read_csv(metadata_path)\n",
        "\n",
        "# Ensure you have the required columns\n",
        "print(df.head())\n",
        "\n",
        "# Convert one-hot encoding to single class column\n",
        "df['dx'] = df[['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']].idxmax(axis=1)\n",
        "\n",
        "# Now apply label mapping\n",
        "label_mapping = {\"MEL\": 0, \"NV\": 1, \"BCC\": 2, \"AKIEC\": 3, \"BKL\": 4, \"DF\": 5, \"VASC\": 6}\n",
        "df['label'] = df['dx'].map(label_mapping)\n",
        "df = df.dropna(subset=['label'])  # Remove rows with missing labels\n",
        "\n",
        "# Define the image directory\n",
        "image_dir = os.path.join(dataset_path, \"images\")  # Adjust if needed\n",
        "\n",
        "# Splitting Data Stratified\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['dx'], random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['dx'], random_state=42)\n",
        "\n",
        "# Function to move images to appropriate folders\n",
        "def move_images(df, source_dir, dest_dir):\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "    for img_name in df['image']:\n",
        "        src = os.path.join(source_dir, img_name + \".jpg\")  # Adjust extension if needed\n",
        "        dest = os.path.join(dest_dir, img_name + \".jpg\")\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy(src, dest)\n",
        "\n",
        "# Create directories\n",
        "move_images(train_df, image_dir, \"dataset/train\")\n",
        "move_images(val_df, image_dir, \"dataset/val\")\n",
        "move_images(test_df, image_dir, \"dataset/test\")\n",
        "\n",
        "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWkjDgbCcqoC",
        "outputId": "8722e779-dfad-4a0f-d038-c73207dc6210"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          image  MEL   NV  BCC  AKIEC  BKL   DF  VASC\n",
            "0  ISIC_0024306  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
            "1  ISIC_0024307  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
            "2  ISIC_0024308  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
            "3  ISIC_0024309  0.0  1.0  0.0    0.0  0.0  0.0   0.0\n",
            "4  ISIC_0024310  1.0  0.0  0.0    0.0  0.0  0.0   0.0\n",
            "Train size: 7210, Val size: 802, Test size: 2003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "classes = np.unique(train_df['label'])\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df['label'])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n"
      ],
      "metadata": {
        "id": "19Dh3UfxvP9f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove leakage by image ID\n",
        "def remove_duplicates(df1, df2, key='image'):\n",
        "    common = set(df1[key]).intersection(set(df2[key]))\n",
        "    return df2[~df2[key].isin(common)]\n",
        "\n",
        "test_df = remove_duplicates(train_df, test_df)\n",
        "val_df = remove_duplicates(train_df, val_df)\n",
        "test_df = remove_duplicates(val_df, test_df)\n",
        "\n",
        "# Patient-level leakage removal\n",
        "if 'patient_id' in df.columns:\n",
        "    train_patients = set(train_df['patient_id'])\n",
        "    val_df = val_df[~val_df['patient_id'].isin(train_patients)]\n",
        "    test_df = test_df[~test_df['patient_id'].isin(train_patients)]\n",
        "    test_df = test_df[~test_df['patient_id'].isin(set(val_df['patient_id']))]"
      ],
      "metadata": {
        "id": "hXJCbR7XcvuV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directories\n",
        "def create_dirs(base='dataset'):\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        for cls in label_mapping:\n",
        "            path = os.path.join(base, split, cls)\n",
        "            os.makedirs(path, exist_ok=True)\n",
        "\n",
        "create_dirs()"
      ],
      "metadata": {
        "id": "bIBX8hpWcx6s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move and check images\n",
        "corrupt_images = []\n",
        "\n",
        "def move_and_check(df, split, source_dir, dest_base):\n",
        "    for _, row in df.iterrows():\n",
        "        img_name = row['image'] + \".jpg\"\n",
        "        label = row['dx']\n",
        "        src = os.path.join(source_dir, img_name)\n",
        "        dest = os.path.join(dest_base, split, label, img_name)\n",
        "        try:\n",
        "            # Validate image\n",
        "            with Image.open(src) as img:\n",
        "                img.verify()  # Will raise error if corrupt\n",
        "            shutil.copy(src, dest)\n",
        "        except Exception as e:\n",
        "            corrupt_images.append(img_name)\n",
        "\n",
        "move_and_check(train_df, 'train', image_dir, 'dataset')\n",
        "move_and_check(val_df, 'val', image_dir, 'dataset')\n",
        "move_and_check(test_df, 'test', image_dir, 'dataset')\n",
        "\n",
        "print(f\"Total corrupt images found and skipped: {len(corrupt_images)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFvfjrdfc0aH",
        "outputId": "54bbb9ae-1b28-4aaf-e236-261ac964d6cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total corrupt images found and skipped: 10015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Image augmentation functions\n",
        "def random_rotation(image):\n",
        "    return image.rotate(random.uniform(-30, 30))\n",
        "\n",
        "def random_flip(image):\n",
        "    if random.random() > 0.5:\n",
        "        return image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    return image\n",
        "\n",
        "def random_crop(image, output_size=(224, 224)):\n",
        "    width, height = image.size\n",
        "    left = random.randint(0, width // 4)\n",
        "    top = random.randint(0, height // 4)\n",
        "    right = width - random.randint(0, width // 4)\n",
        "    bottom = height - random.randint(0, height // 4)\n",
        "    return image.crop((left, top, right, bottom)).resize(output_size)\n",
        "\n",
        "\n",
        "def preprocess_and_save(df, source_dir, dest_dir, image_column, label_column):\n",
        "    processed_count = 0\n",
        "    missing_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        img_name = row[image_column]\n",
        "        label = str(row[label_column])  # e.g. '0', '1', ..., '6'\n",
        "        class_dir = os.path.join(dest_dir, label)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "        src = os.path.join(source_dir, img_name)\n",
        "        dest = os.path.join(class_dir, img_name)\n",
        "\n",
        "        # Check if file exists with extensions\n",
        "        if not os.path.exists(src):\n",
        "            if os.path.exists(src + \".jpg\"):\n",
        "                src += \".jpg\"\n",
        "                dest += \".jpg\"\n",
        "            elif os.path.exists(src + \".png\"):\n",
        "                src += \".png\"\n",
        "                dest += \".png\"\n",
        "            else:\n",
        "                print(f\"❌ Missing during preprocessing: {src}\")\n",
        "                missing_count += 1\n",
        "                continue\n",
        "\n",
        "        try:\n",
        "            with Image.open(src) as img:\n",
        "                img = img.convert(\"RGB\")\n",
        "                img.save(dest)\n",
        "                processed_count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {src}: {e}\")\n",
        "            error_count += 1\n",
        "\n",
        "    print(f\"✅ Processed images: {processed_count}\")\n",
        "    print(f\"❌ Missing images: {missing_count}\")\n",
        "    print(f\"❌ Errors during processing: {error_count}\")\n",
        "\n",
        "# Re-run for the test set\n",
        "preprocess_and_save(test_df, \"dataset/test\", \"dataset/preprocessed_test\", \"image\", \"label\")\n",
        "\n",
        "\n",
        "# Apply preprocessing\n",
        "preprocess_and_save(train_df, \"dataset/train\", \"dataset/preprocessed_train\", \"image\",\"label\" )\n",
        "preprocess_and_save(test_df, \"dataset/test\", \"dataset/preprocessed_test\", \"image\",\"label\")\n",
        "preprocess_and_save(val_df, \"dataset/val\", \"dataset/preprocessed_val\", \"image\",\"label\")\n",
        "\n",
        "print(\"Preprocessing complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx6zju_7dnLk",
        "outputId": "e2e70a0b-9b16-41f5-ee42-2599a3779cb3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Processed images: 2003\n",
            "❌ Missing images: 0\n",
            "❌ Errors during processing: 0\n",
            "✅ Processed images: 7210\n",
            "❌ Missing images: 0\n",
            "❌ Errors during processing: 0\n",
            "✅ Processed images: 2003\n",
            "❌ Missing images: 0\n",
            "❌ Errors during processing: 0\n",
            "✅ Processed images: 802\n",
            "❌ Missing images: 0\n",
            "❌ Errors during processing: 0\n",
            "Preprocessing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "def count_images_per_class(base_dir):\n",
        "    counts = {}\n",
        "    for class_name in os.listdir(base_dir):\n",
        "        class_dir = os.path.join(base_dir, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            counts[class_name] = len(os.listdir(class_dir))\n",
        "    return counts\n",
        "\n",
        "print(count_images_per_class(\"dataset/preprocessed_train\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kmq4RoReSNb",
        "outputId": "eaf9b5e6-74e8-4d60-e6b9-a3f96a25f81f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'6': 102, '4': 791, '2': 370, '3': 236, '0': 801, '1': 4827, '5': 83}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load pre-trained ResNet18 (only ~11.7 million parameters)\n",
        "def build_model(num_classes):\n",
        "    model = models.resnet18(pretrained=True)\n",
        "\n",
        "    # Replace the final classification layer to match HAM10000 classes\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Count total trainable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Build model\n",
        "num_classes = 7  # MEL, NV, BCC, AKIEC, BKL, DF, VASC\n",
        "model = build_model(num_classes).to(device)\n",
        "\n",
        "# Print model summary\n",
        "total_params = count_parameters(model)\n",
        "print(f\"✅ Total trainable parameters: {total_params:,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63Ekcp8-ktIr",
        "outputId": "f4be1d2f-c534-4bed-aa3f-0cf146caa4a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total trainable parameters: 11,180,103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader, class_names):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Overall Accuracy\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"\\n✅ Test Accuracy: {acc:.4f}\\n\")\n",
        "\n",
        "    # Classification Report (Precision, Recall, F1 per class)\n",
        "    print(\"🔍 Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"📊 Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "8tlanIDSmBfM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "# Define the transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to match ResNet input size\n",
        "    transforms.ToTensor(),          # Convert image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
        "])\n",
        "\n",
        "\n",
        "# Datasets\n",
        "train_dataset = datasets.ImageFolder(\"dataset/preprocessed_train\", transform=transform)\n",
        "val_dataset = datasets.ImageFolder(\"dataset/preprocessed_val\", transform=transform)\n",
        "test_dataset = datasets.ImageFolder(\"dataset/preprocessed_test\", transform=transform)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=5,pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=5,pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=5,pin_memory=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4EfKcayySyS",
        "outputId": "c49f5bae-cf71-4d87-9ee8-f392aaa78283"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.optim as optim\n",
        "\n",
        "# def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, patience=3):\n",
        "#     best_val_loss = float('inf')\n",
        "#     early_stop_counter = 0\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         model.train()\n",
        "#         running_loss = 0.0\n",
        "#         correct = 0\n",
        "#         total = 0\n",
        "\n",
        "#         for images, labels in train_loader:\n",
        "#             images, labels = images.to(device), labels.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "#             outputs = model(images)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item() * images.size(0)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             correct += (preds == labels).sum().item()\n",
        "#             total += labels.size(0)\n",
        "\n",
        "#         train_loss = running_loss / total\n",
        "#         train_acc = correct / total\n",
        "\n",
        "#         # Validation\n",
        "#         model.eval()\n",
        "#         val_loss = 0.0\n",
        "#         correct = 0\n",
        "#         total = 0\n",
        "#         with torch.no_grad():\n",
        "#             for images, labels in val_loader:\n",
        "#                 images, labels = images.to(device), labels.to(device)\n",
        "#                 outputs = model(images)\n",
        "#                 loss = criterion(outputs, labels)\n",
        "#                 val_loss += loss.item() * images.size(0)\n",
        "#                 _, preds = torch.max(outputs, 1)\n",
        "#                 correct += (preds == labels).sum().item()\n",
        "#                 total += labels.size(0)\n",
        "\n",
        "#         val_loss /= total\n",
        "#         val_acc = correct / total\n",
        "\n",
        "#         print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "#         # Early stopping\n",
        "#         if val_loss < best_val_loss:\n",
        "#             best_val_loss = val_loss\n",
        "#             early_stop_counter = 0\n",
        "#             torch.save(model.state_dict(), 'best_model.pth')\n",
        "#             print(\"✅ Model saved!\")\n",
        "#         else:\n",
        "#             early_stop_counter += 1\n",
        "#             if early_stop_counter >= patience:\n",
        "#                 print(\"⏹️ Early stopping triggered.\")\n",
        "#                 break\n",
        "\n",
        "#     print(\"🎉 Training complete!\")\n",
        "\n",
        "# # Define optimizer and train the model\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20, patience=5)\n"
      ],
      "metadata": {
        "id": "9DbfovTT9bah"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # Training function optimized for speed with AMP and fewer syncs\n",
        "# def train_model(model, train_loader, val_loader, num_epochs=10, accumulation_steps=4):\n",
        "#     model.train()\n",
        "#     optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "#     scaler = torch.cuda.amp.GradScaler()  # Mixed precision training\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model.to(device)\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         model.train()\n",
        "#         running_loss = 0.0\n",
        "#         optimizer.zero_grad(set_to_none=True)  # Reduce memory usage\n",
        "\n",
        "#         for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
        "#             inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "#             with torch.cuda.amp.autocast():  # Enable mixed precision\n",
        "#                 outputs = model(inputs)\n",
        "#                 loss = criterion(outputs, labels) / accumulation_steps  # Normalize loss\n",
        "\n",
        "#             scaler.scale(loss).backward()\n",
        "\n",
        "#             if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
        "#                 scaler.step(optimizer)\n",
        "#                 scaler.update()\n",
        "#                 optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "#             running_loss += loss.item() * accumulation_steps\n",
        "\n",
        "#         print(f\"Epoch {epoch+1} Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "#         # Validation loop with minimal computation overhead\n",
        "#         model.eval()\n",
        "#         correct, total = 0, 0\n",
        "#         with torch.no_grad():\n",
        "#             for inputs, labels in val_loader:\n",
        "#                 inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "#                 outputs = model(inputs)\n",
        "#                 _, preds = torch.max(outputs, 1)\n",
        "#                 correct += (preds == labels).sum().item()\n",
        "#                 total += labels.size(0)\n",
        "\n",
        "#         print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "#     print(\"Training complete.\")\n",
        "\n",
        "# # Call the function to start training\n",
        "# train_model(model, train_loader, val_loader, num_epochs=10, accumulation_steps=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "MFaoxC3I-lpk",
        "outputId": "3f45336f-0204-4b9c-c730-25d50b6fc099"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-d746877d9318>:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()  # Mixed precision training\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "Epoch 1/10:   0%|          | 0/451 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-16-d746877d9318>:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():  # Enable mixed precision\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "Epoch 1/10:  10%|▉         | 44/451 [03:21<31:06,  4.59s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d746877d9318>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Call the function to start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-d746877d9318>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, accumulation_steps)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0maccumulation_steps\u001b[0m  \u001b[0;31m# Normalize loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import albumentations as A\n",
        "# from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# train_transform = A.Compose([\n",
        "#     A.Resize(224, 224),\n",
        "#     A.HorizontalFlip(),\n",
        "#     A.RandomBrightnessContrast(),\n",
        "#     A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15),\n",
        "#     A.Normalize(),\n",
        "#     ToTensorV2(),\n",
        "# ])\n"
      ],
      "metadata": {
        "id": "aytl7nSLyGvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchvision.datasets import ImageFolder\n",
        "# import cv2\n",
        "\n",
        "# class AlbumentationsImageFolder(ImageFolder):\n",
        "#     def __init__(self, root, transform=None):\n",
        "#         super().__init__(root)\n",
        "#         self.albumentations_transform = transform\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         path, label = self.samples[index]\n",
        "#         image = cv2.imread(path)\n",
        "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#         if self.albumentations_transform:\n",
        "#             image = self.albumentations_transform(image=image)['image']\n",
        "\n",
        "#         return image, label\n"
      ],
      "metadata": {
        "id": "2N5ScVeY5tr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# from torch.utils.data import DataLoader\n",
        "# from tqdm import tqdm\n",
        "# import cv2\n",
        "# import os\n",
        "\n",
        "# # Use your custom dataset\n",
        "# # Ensure this dataset uses cv2 and albumentations as discussed\n",
        "# train_dataset = AlbumentationsImageFolder(root='dataset/preprocessed_train', transform=train_transform)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)  # Set workers to 0 for clear profiling\n",
        "\n",
        "# # Test just 1 batch to analyze bottlenecks\n",
        "# print(\"🔍 Running bottleneck test on a single batch...\\n\")\n",
        "\n",
        "# start_total = time.time()\n",
        "\n",
        "# for i, (images, labels) in enumerate(train_loader):\n",
        "#     if i >= 1:  # Only run one batch\n",
        "#         break\n",
        "\n",
        "#     print(f\"\\n✅ Loaded batch {i + 1}\")\n",
        "#     print(f\"🧪 Images shape: {images.shape}\")\n",
        "#     print(f\"🧪 Labels: {labels}\")\n",
        "\n",
        "# end_total = time.time()\n",
        "\n",
        "# print(f\"\\n⏱️ Total time to load 1 batch: {end_total - start_total:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "zcf0VPC77XSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"\\n🔍 Profiling per-sample in dataset...\\n\")\n",
        "# sample_times = []\n",
        "\n",
        "# for idx in tqdm(range(10)):  # Test on just 10 samples\n",
        "#     start = time.time()\n",
        "#     image, label = train_dataset[idx]\n",
        "#     end = time.time()\n",
        "#     sample_times.append(end - start)\n",
        "\n",
        "# avg_time = sum(sample_times) / len(sample_times)\n",
        "# print(f\"\\n⏱️ Avg time per sample: {avg_time:.4f} seconds (~{1/avg_time:.2f} samples/sec)\")\n"
      ],
      "metadata": {
        "id": "WyW2rY9r7awX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.optim import Adam\n",
        "# from torch.utils.data import DataLoader\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # Define train and val datasets\n",
        "# # train_dataset = ImageFolder('dataset/preprocessed_train', transform=train_transform)\n",
        "# # val_dataset = ImageFolder('dataset/preprocessed_val', transform=test_transforms)\n",
        "\n",
        "# # train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "# # val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "# train_dataset = AlbumentationsImageFolder(root='dataset/preprocessed_train', transform=train_transform)\n",
        "# val_dataset = AlbumentationsImageFolder(root='dataset/preprocessed_val', transform=test_transforms)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4,pin_memory=True)\n",
        "\n",
        "\n",
        "# optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "# num_epochs = 10  # increase for better performance\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     running_loss = 0.0\n",
        "\n",
        "#     for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "#         images, labels = images.to(device), labels.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         outputs = model(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_loss += loss.item()\n",
        "\n",
        "#     print(f\"🔥 Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "7pbb_7CP5e42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your DataLoader for the preprocessed test set\n",
        "# Example: test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "test_dataset = ImageFolder(root='dataset/preprocessed_test', transform=test_transforms)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Define class labels (match your label mapping)\n",
        "class_names = [\"MEL\", \"NV\", \"BCC\", \"AKIEC\", \"BKL\", \"DF\", \"VASC\"]\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader, class_names)\n"
      ],
      "metadata": {
        "id": "pbkjISB1mC7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THIS IS WITHOUT BONUS\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def conv_block(in_c, out_c):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "\n",
        "        self.enc1 = conv_block(in_channels, 64)\n",
        "        self.enc2 = conv_block(64, 128)\n",
        "        self.enc3 = conv_block(128, 256)\n",
        "        self.enc4 = conv_block(256, 512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        self.bottleneck = conv_block(512, 1024)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.dec4 = conv_block(1024, 512)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec3 = conv_block(512, 256)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = conv_block(256, 128)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = conv_block(128, 64)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool(e1))\n",
        "        e3 = self.enc3(self.pool(e2))\n",
        "        e4 = self.enc4(self.pool(e3))\n",
        "\n",
        "        b = self.bottleneck(self.pool(e4))\n",
        "\n",
        "        d4 = self.upconv4(b)\n",
        "        d4 = torch.cat((e4, d4), dim=1)\n",
        "        d4 = self.dec4(d4)\n",
        "\n",
        "        d3 = self.upconv3(d4)\n",
        "        d3 = torch.cat((e3, d3), dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "\n",
        "        d2 = self.upconv2(d3)\n",
        "        d2 = torch.cat((e2, d2), dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        d1 = self.upconv1(d2)\n",
        "        d1 = torch.cat((e1, d1), dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        return torch.sigmoid(self.out_conv(d1))\n"
      ],
      "metadata": {
        "id": "tANzdb2wobP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs_class, outputs_mask = model(images)\n",
        "\n",
        "# # Classification loss\n",
        "# loss_cls = criterion_class(outputs_class, labels)\n",
        "\n",
        "# # Segmentation loss\n",
        "# loss_seg = criterion_seg(outputs_mask, masks)\n",
        "\n",
        "# # Total loss (you can tune the weights)\n",
        "# loss = loss_cls + loss_seg\n"
      ],
      "metadata": {
        "id": "tKlkGKGpHEi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.image_names = os.listdir(image_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_names[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.image_names[idx].replace(\".jpg\", \"_mask.png\"))  # adjust as needed\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")  # binary mask\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=np.array(image), mask=np.array(mask))\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        return image, mask\n"
      ],
      "metadata": {
        "id": "6obxU8OtorKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Dice Coefficient: 2 * |A ∩ B| / (|A| + |B|)\n",
        "def dice_score(pred, target, smooth=1e-6):\n",
        "    pred = pred.view(-1)\n",
        "    target = target.view(-1)\n",
        "    intersection = (pred * target).sum()\n",
        "    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
        "\n",
        "# IoU: |A ∩ B| / |A ∪ B|\n",
        "def iou_score(pred, target, smooth=1e-6):\n",
        "    pred = pred.view(-1)\n",
        "    target = target.view(-1)\n",
        "    intersection = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum() - intersection\n",
        "    return (intersection + smooth) / (union + smooth)\n"
      ],
      "metadata": {
        "id": "pneggNnHyVRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    dice_scores = []\n",
        "    iou_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = (outputs > 0.5).float()  # Threshold for binary masks\n",
        "            #print(preds.unique(), true.unique())\n",
        "            for pred, true in zip(preds, masks):\n",
        "                pred = pred.view(-1)\n",
        "                true = true.view(-1)\n",
        "                # Normalize to float32 binary masks\n",
        "                pred = pred.float()\n",
        "                true = (true > 0.5).float()\n",
        "\n",
        "                dice = dice_score(pred, true)\n",
        "                iou = iou_score(pred, true)\n",
        "                dice_scores.append(dice.item())\n",
        "                iou_scores.append(iou.item())\n",
        "\n",
        "    avg_dice = np.mean(dice_scores)\n",
        "    avg_iou = np.mean(iou_scores)\n",
        "\n",
        "    print(f\"\\n✅ Test Dice Coefficient: {avg_dice:.4f}\")\n",
        "    print(f\"✅ Test IoU Score: {avg_iou:.4f}\")\n",
        "    return avg_dice, avg_iou\n",
        "dice, iou = evaluate_model(model, test_loader, device)\n",
        "\n",
        "#dice score is low, >0.7 is good, IoU score was too high,should be from 0-1, >0.5 is ok,, >0.8 is good\n"
      ],
      "metadata": {
        "id": "LZQ55hYwyWvY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}